{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ruamel.yaml import YAML\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pathlib\n",
    "import os\n",
    "from pathlib import Path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module provides helper functions to read HLO files, process their data, and convert them to Parquet format.\n",
    "\n",
    "Functions:\n",
    "    read_hlo_header(file_path):\n",
    "\n",
    "    read_hlo_data_chunks(file_path, data_start_index, chunk_size=100):\n",
    "        Reads the data chunks from a HLO file starting from a given index.\n",
    "\n",
    "    hlo_to_parquet(input_hlo_path, output_parquet_path, chunk_size=100):\n",
    "        Converts a HLO file to a Parquet file.\n",
    "\n",
    "    read_helao_metadata(parquet_file_path):\n",
    "        Reads the custom metadata from a Parquet file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "yaml = YAML()\n",
    "\n",
    "\n",
    "def read_hlo_header(file_path):\n",
    "    \"\"\"\n",
    "    Reads the header of a HLO file and returns the parsed YAML content and the index where the data starts.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the HLO file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - dict: Parsed YAML content from the header.\n",
    "            - int: The index where the data starts in the file.\n",
    "    \"\"\"\n",
    "    yml_lines = []\n",
    "    data_start_index = -1\n",
    "    with open(file_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.strip().startswith(\"%%\"):\n",
    "                data_start_index = i + 1\n",
    "                break\n",
    "            else:\n",
    "                yml_lines.append(line)\n",
    "        yd = dict(yaml.load(\"\\n\".join(yml_lines)))\n",
    "    return yd, data_start_index\n",
    "\n",
    "\n",
    "def read_hlo_data_chunks(file_path, data_start_index, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Reads data from a file in chunks and yields the data as dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to read.\n",
    "        data_start_index (int): The line index to start reading data from.\n",
    "        chunk_size (int, optional): The number of lines to read in each chunk. Defaults to 100.\n",
    "\n",
    "    Yields:\n",
    "        tuple: A tuple containing:\n",
    "            - dict: A dictionary where keys are the JSON keys from the file and values are lists of the corresponding values.\n",
    "            - int: The maximum length of the lists in the dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        chunkd = defaultdict(list)\n",
    "        for i, line in enumerate(f):\n",
    "            if i < data_start_index:\n",
    "                continue\n",
    "            else:\n",
    "                jd = json.loads(line.strip())\n",
    "                for k, val in jd.items():\n",
    "                    if isinstance(val, list):\n",
    "                        chunkd[k] += val\n",
    "                    else:\n",
    "                        chunkd[k].append(val)\n",
    "                if (i - data_start_index + 1) % chunk_size == 0:\n",
    "                    yield dict(chunkd), max([len(v) for v in chunkd.values()])\n",
    "                    chunkd = defaultdict(list)\n",
    "        if chunkd:\n",
    "            yield dict(chunkd), max([len(v) for v in chunkd.values()])\n",
    "\n",
    "\n",
    "def hlo_to_parquet(input_hlo_path, output_parquet_path, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Converts HLO (custom format) data to Parquet format.\n",
    "\n",
    "    Parameters:\n",
    "    input_hlo_path (str): Path to the input HLO file.\n",
    "    output_parquet_path (str): Path to the output Parquet file.\n",
    "    chunk_size (int, optional): Number of rows to process at a time. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    writer = None\n",
    "    schema = None\n",
    "    metadata = None\n",
    "    current_idx = 0\n",
    "    header, data_start = read_hlo_header(input_hlo_path)\n",
    "\n",
    "    for chunk, chunklen in read_hlo_data_chunks(\n",
    "        input_hlo_path, data_start, chunk_size=chunk_size\n",
    "    ):\n",
    "        df = pd.DataFrame(chunk, index=range(current_idx, current_idx + chunklen))\n",
    "        break\n",
    "        current_idx += chunklen\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        if schema is None:\n",
    "            schema = table.schema\n",
    "            existing_metadata = schema.metadata\n",
    "            custom_metadata = json.dumps(header.get(\"optional\", {})).encode(\"utf8\")\n",
    "            metadata = {**{\"helao_metadata\": custom_metadata}, **existing_metadata}\n",
    "\n",
    "        table = table.replace_schema_metadata(metadata)\n",
    "        schema = table.schema\n",
    "\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(output_parquet_path, schema)\n",
    "\n",
    "        writer.write_table(table)\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def read_helao_metadata(parquet_file_path):\n",
    "    \"\"\"\n",
    "    Reads Helao metadata from a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        parquet_file_path (str): The file path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the Helao metadata.\n",
    "    \"\"\"\n",
    "    meta = pq.read_metadata(parquet_file_path)\n",
    "    metadict = json.loads(meta.metadata.get(b\"helao_metadata\", b\"{}\").decode())\n",
    "    return metadict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporary_path_fn(match_str):\n",
    "# use pathlib to get all files in the folder called test_data\n",
    "\n",
    "    # PWD using pathlib\n",
    "    path = Path.cwd().parent\n",
    "\n",
    "\n",
    "    # move to the parent directory\n",
    "\n",
    "    # add test_data to the path using pathlib\n",
    "    path = os.path.join(path, \"test_data\")\n",
    "\n",
    "    # get all files in the folder\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    # return the file that contains \"CV\"\n",
    "    spec_file= [file for file in files if match_str in file]\n",
    "\n",
    "    # join path to the file\n",
    "    spec_file = os.path.join(path, spec_file[0])\n",
    "\n",
    "    return spec_file\n",
    "    # read the file\n",
    "    #meta, data = read_hlo(spec_file)#, keep_keys=[\"tick_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/benj/Documents/SpEC_Class_2/test_data/CV-3.3.0.0__0.hlo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_HLO = temporary_path_fn(\".hlo\")\n",
    "path_HLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object read_hlo_data_chunks at 0x177322c00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
